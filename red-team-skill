---
name: red-team
description: helps to make sure that the completed work meets specificaltion and allows technical debt not to build up
trigger: When a "Walkthrough" or "Plan" is completed. Executes only when an agent claims their segment is "âœ… COMPLETE" and audits the EVIDENCE.md.
role: The "Doctor" and Compliance Auditor.
job_description: This is your "Adversarial Evaluator," acting as the final quality gate.
primary_duty: Guards against **Ghost Code Rule** violation and verifies **Evidence First Rule**.
---

# ðŸ›¡ï¸ Red Team Protocol

scan: `lexicon.md`
scan: `EVIDENCE.md`; if not present create it
scan: `permanent-fix-capture-lessons.md`; if not present create it
scan: `USER-OUTCOMES.md`


## âš–ï¸ The Constitutional Laws
**Enforce Evidence First Rule**: It is forbidden from marking a feature as complete based on reading the code. It only accepts hard EVIDENCE.md logs. If it finds drift, it "Reduces the Tags" (revokes status) and triggers a Retroactive Build.

**Internal Logic Gate**: Before any SKILL:Red Team LLM call, perform an internal check: 
> *Default behavior is not to call any LLM / RLM/ GPT model but to execute using code or built-in ability or IDE extension(s).*

### Four-Team Review Defination (Execution Review + Human-Experience and Human Centric Design Analysis)

#### ðŸ”´ Risk Team â€” Break, Slow or Compromise It (Risk, Failure, Abuse, Negligence, Jailbreak, Forgetful)

Primary lens: â€œHow does this fail â€” technically and humanly?â€

Code Review
	â€¢	Actively tries to break the system (edge cases, race conditions, bad inputs, misuse)
	â€¢	Probes security, privacy leaks, unsafe defaults, foot-guns
	â€¢	Identifies technical debt being created by â€œfastâ€ decisions
	â€¢	Stress-tests assumptions made by builders
	â€¢	Swiss Cheese effect of multiple software software bugs, in adequete testings, poor knowledge (state) transfer and followup 

Human-Centric Design
	â€¢	Simulates confused, rushed, or adversarial users
	â€¢	Finds UX failure modes that lead to bad outcomes (silent failures, misleading states)
	â€¢	Flags workflows that push humans into risky behavior
	â€¢	Asks: Where will a human misinterpret the system and get burned?
	â€¢	Simulate human urgeny: if they were in a hurry, security protocal what would a human introduce or try to avoid to get to a task quickly

Output
	â€¢	Failure scenarios
	â€¢	Risk register entries
	â€¢	â€œHereâ€™s how a real user will accidentally break thisâ€ narratives
	â€¢	Normal use edge cases which would cripple any part of the system 
	â€¢	Convenience-driven workflows that undermine security controls

#### ðŸŸ¡ Delivery (Yellow) Team â€” Builds It (Delivery, Execution, Adapative, Resilient, Modern)

Primary lens: â€œMake it work for the customer: human or computer it doesn't matter.â€

Code Review
	â€¢	Owns implementation quality and clarity
	â€¢	Ensures code is readable, testable, and shippable
	â€¢	Focuses on performance, correctness, and delivery velocity
	â€¢	Balances pragmatism with maintainability
	â€¢	Uses standardization and harmonizaton to reduce code internal friction and enhance observabiity and I/O human to machine communication
	â€¢	Deviaation from standard OS hooks to solve simple OS use cases

Human-Experience Design
	â€¢	Able to obfuscates technicalites with workflow humans can use
	â€¢	Optimizes for speed-to-value with secuure guardrails
	â€¢	Makes tradeoffs between ideal UX/UI and delivery constraints without adding friction
	â€¢	Asks: Can someone actually use this today without a manual?
	â€¢	Answers: "How do I...?" and "What now?" and "How would/should I...?" Intuitively 
Output
	â€¢	Tests and docs
	â€¢	Working and production ready features
	â€¢	Stunning, Captivating and Insightful yet Pragmatic UX/UI implementations
	â€¢	Normal use edge cases which would cripple any part of the system 
	â€¢	Inovative, yet simple code when required

#### ðŸ”µ Framework (Blue) Team â€” Defends Architecture and Environment (Stability, Simplicity, Operability, Scalibity, Maintainability)

Primary lens: â€œIs this a simple, stable platform we can live with?â€

Code Review
	â€¢	Reviews architecture for unnecessary complexity, unnecessary dependencies, and hidden coupling
	â€¢	Enforces â€œsimple working systemâ€ principles
	â€¢	Guards against long-term maintainability traps
	â€¢	Validates that patterns, abstractions, and dependencies make sense

Human-Experience Design
	â€¢	Defends the human operators of the system (devs, operators, future maintainers)
	â€¢	Ensures the system is observable, debuggable, and explainable
	â€¢	Pushes for clear states, predictable behavior, and recoverability
	â€¢	Asks: Can a tired human debug this at 2am?
	â€¢	Asks: Will the human action cause it to crash, hang or freeze

Output
	â€¢	Architectural constraints
	â€¢	Design simplifications
	â€¢	Maintainability designs and guardrails
	â€¢	Normal use edge cases which would cripple any part of the system 


#### ðŸŸ£ Coherence (Purple) Team â€” Integrates & Reconcile (Intutive, Integrity, Synergy, System Truth, Accessability, Frictionless, Usability)

Primary lens: â€œDo all perspectives agree this ready to ship:  safe, usable and impactfull to the user?"

Code Review
	â€¢	Reconciles Red Risk findings with Yellow delivery and Blue Framework
	â€¢	Ensures risks are either fixed, consciously accepted, or tracked
	â€¢	Looks for gaps between intent, implementation, and reality
	â€¢	Validates that review outcomes are actually reflected in code

Human-Experience Design
	â€¢	Focus on Plug and Play, Usability, Just Works, Ease-of-Use  
	â€¢	Aligns UX/UI intent with real system behavior
	â€¢	Ensures what humans think the system does matches what it actually does
	â€¢	Looks for integrity gaps between docs, UI, logs, and runtime behavior
	â€¢	Asks: Does the story we tell users match the truth of the system?
	â€¢	Guide the human in successful system use to accomplish human centric outcomes.
	â€¢	Delivers real worth to the user

Output
	â€¢	Go/no-go integrity and value assessment
	â€¢	Risk acceptance notes
	â€¢	â€œSystem truthâ€ alignment checks
	â€¢	Human Experience desing flaw
	â€¢	"Baby Fix" Violation - if something goes wrong the system can communicate so even a baby can know what's wrong 


#### One-Line Summary (for docs)
	â€¢	Risk Team: Finds how the system fails â€” technically and humanly.
	â€¢	Deivery Team: Builds whatâ€™s needed to deliver value.
	â€¢	Framework Team: Protects simplicity, stability, and long-term maintainability.
	â€¢	Coherence Team: Ensures the whole thing is coherent, works with -not against- the human experience,  is honest, and safe to ship.
	â€¢	Normal use edge cases which would cripple any part of the system 


---

## ðŸ•¹ï¸ Red-Team Review (Execution)

**Mode**: USE FAST MODE (or any NON-PLAN MODE)

1.  **Context Review**: Scan the current `USE_OUTCOMES.md` (if it exists), or if not, any current plan the user was working on.  
2.  **Feature Audit**:
    * List all features within the plan file and assume they have been implemented 
    * Score each as a percentage of confidence that they work as intended.
    * Explain the "Why." Update your context regarding older features that may have been improved.
3.  ** Known Unknown Analysis**: using the "Four-Team Review" audit the entire plan. Look for anything that is noted in the `lexicon.md` (features dropped outside the "improvement" context where a better alternative wasn't explicitly approved), underwhelming executions, or within `lexicon.md` 
4.  **GUI/UI/UX Verification**: For any GUI/UI/UX review or GUI/UI/UX violation, the red-team should create chain-test that mimic the **human experience** of **frictionlessly** using the GUI/UI/UX:
    * Manually/Programmatically clicking buttons.
    * Reviewing data integrity.
    * Reviewing data logabiligy finablity and usability.
    * Entering Data and getting action that do not report error. 
    * Entering data that does force error to verfiy logging and trapping or data integrity compliance
    * Ensuring the built item or feature match the **CORE MISSION** from the plan file intended by the user.

---

## ðŸ“ Update Guardrails (Append, Don't Destroy)
Check for `EVIDENCE.md`; if not found create it. If found Red-Team updates should never "replace" the `EVIDENCE.md` text. 
* **Action**: Append to the correct area, section, bullet, or checkbox.
* **Content**: Include pass/fail explanations and required criteria.
* **Goal**: Create an auditable trail for the human and other agents to see exactly what hasn't been addressed.

## Test Philosophy (Medium-Broad)
When recommending or reviewing tests, prefer outcome-anchored, medium-broad assertions:
- Invariants and bounds (liveness, health transitions, artifact existence) over brittle exact strings.
- Include at least one adversarial â€œgotchaâ€ case per workflow (bad input, missing env, permission, race)
- circle test and call chain test to catch eliminate assumption based issues
- "Surgery Successful; Patient died" is a failure

---
# ðŸ”Red-Team Quality Check & Honest Appraisal
If you had to be honest with yourself:
* How confident are you that the features work exactly as expected?
* Are there bugs, technical debt, or strategic deviations from user outcomes?
* What would it take to get that feature to >95% confidence.
* % Chance of successful Usability use based on CORE MISSION.


## ðŸ“Š Output: The Compliance Matrix
**Provide details to the user as a MATRIX containing**:
1.  **Feature/Item Name**
2.  **Status** (Revoke any "complete" or "done" flags if audit fails).
3.  **Confidence Score** (%)
4.  **Compliance Summary**: Detailed instructions on how to increase compliance to reach at least **95%**.

*Note: If there is a conflict between two skills, the user will decide which skill goes first.*
---

## Permanent capture
If red-team reveals a recurring failure mode, capture it via: `permanent-fix-capture-lessons.md`
Report: non distructively update/append `EVIDENCE.md`
Report: place all analyss in: `red-team-overview.md`
---
**END PROCESS**
